# -*- coding: utf-8 -*-
"""newtrain.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1akPJS9A_i0ma93IhP8M1NJkK8BbUIefJ


!pip install --upgrade --force-reinstall --no-deps albumentations

!pip install pipreqs opencv-python-headless imgaug libtiff rasterio
"""

from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np
import time
import sys
from itertools import product
import cv2
import os
import matplotlib.pyplot as plt
from libtiff import TIFF
import random, torch, os, numpy as np
import torch.nn as nn
import copy
from torch.utils.data import TensorDataset, DataLoader
import torch.optim as optim
from tqdm.notebook import tqdm
from torchvision.utils import save_image
from torch.nn import functional as F
import rasterio
from sklearn.preprocessing import MinMaxScaler
import re
import matplotlib.pyplot as plt
from sklearn.metrics import precision_score,recall_score,f1_score,jaccard_score,accuracy_score
from PIL import Image
import os
from torch.utils.data import Dataset
import numpy as np
import random, torch, os, numpy as np
import torch.nn as nn
import copy

def mean_normalize(image):
  std_lst = []
  mean_lst = []
  for idx in range(image.shape[2]):
    std_chan = np.std(image[:,:,idx])
    mean_chan = np.mean(image[:,:,idx])
    std_lst.append(std_chan)
    mean_lst.append(mean_chan)

  std_img = np.array([std_lst])
  mean_img = np.array([mean_lst])
  norm_img = (image - mean_img)/std_img
  return std_img, mean_img, norm_img

IMAGE_SIZE = 256

import random
imgname_list = os.listdir('/content/drive/MyDrive/Anshul files/Inria_datasets/images/')
imgname_list = sorted(imgname_list)

mask_list = os.listdir('/content/drive/MyDrive/Anshul files/Inria_datasets/gt/')
mask_list = sorted(mask_list)


tile_sizex = IMAGE_SIZE
tile_sizey = IMAGE_SIZE

min_feature_cnt = 1

i=1

imageseg = dict()

img_folder_path = '/content/drive/MyDrive/Anshul files/Inria_datasets/images/'
for img_name in imgname_list:
  print(img_name)

  img_name = img_folder_path+img_name
  tifinimg = TIFF.open(img_name)
  inimg = tifinimg.read_image()
  TIFF.close(tifinimg)

  img_dtype = type(inimg[0,0,0])

  print(inimg.shape)

  (H,W,Ch) = inimg.shape

  (H_delta,W_delta) = 0,0

  if (H % tile_sizey) != 0:
    H_delta = tile_sizey - (H % tile_sizey)
    #print(H_delta)
  if (W % tile_sizex) != 0:
    W_delta = tile_sizex - (W % tile_sizex)
    #print(W_delta)

  #print(img_dtype)

  IMG_FILL_VALUE = np.max(inimg)

  top, bottom, left, right = 0, H_delta, 0, W_delta

  inimg_new =  np.ones((H+H_delta, W+W_delta,Ch), dtype=img_dtype) * IMG_FILL_VALUE
  inimg_new[:H,:W,:] = inimg

  #_, _, inimg_new = mean_normalize(inimg_new)
  inimg_new = inimg_new/255
  offsets = product(range(0, W, tile_sizex), range(0, H, tile_sizey))

  cnt = 1

  for row_off,col_off in offsets:
    print("Img:{}. (Row:Col) = ({}:{})".format(cnt,row_off,col_off))

    col_start, col_end, row_start, row_end = col_off, col_off+tile_sizey-1, row_off, row_off+tile_sizex-1

    imgtile = inimg_new[col_start:col_end+1,row_start:row_end+1,:]
    imageseg['{}-{}-{}'.format(i, row_off,col_off)] = imgtile

    cnt = cnt + 1

  i=i+1

  if i>2:
    break
    #print(imgtile.shape)

i=1

imageseg2 = dict()
trainx_list = []
trainy_list = []

mask_folder_path = '/content/drive/MyDrive/Anshul files/Inria_datasets/gt/'
for mask_name in mask_list:
  print(mask_name)

  img_name = mask_folder_path+mask_name
  tifinimg = TIFF.open(img_name)
  inmask = tifinimg.read_image()
  TIFF.close(tifinimg)

  mask_dtype = type(inmask[0,0])

  print(mask_dtype.shape)

  (H2,W2) = inmask.shape

  (H_delta2,W_delta2) = 0,0

  if (H2 % tile_sizey) != 0:
    H_delta2 = tile_sizey - (H2 % tile_sizey)
  if (W2 % tile_sizex) != 0:
    W_delta2 = tile_sizex - (W2 % tile_sizex)

  top2, bottom2, left2, right2 = 0, H_delta2, 0, W_delta2
  inmask_new =  cv2.copyMakeBorder(inmask, top2, bottom2, left2, right2, cv2.BORDER_CONSTANT, value=0)
    #print(W_delta)
  mask_minval = np.min(inmask_new)
  mask_maxval = np.max(inmask_new)
  inmask_new = inmask_new / (mask_maxval - mask_minval)

  offsets2 = product(range(0, W2, tile_sizex), range(0, H2, tile_sizey))

  cnt2 = 1
  #print(img_dtype)
  for row_off2,col_off2 in offsets2:
    print("Mask:{}. (Row:Col) = ({}:{})".format(cnt2,row_off2,col_off2))
    col_start2, col_end2, row_start2, row_end2 = col_off2, col_off2+tile_sizey-1, row_off2, row_off2+tile_sizex-1
    imgtile2 = inmask_new[col_start2:col_end2+1,row_start2:row_end2+1]

    imgtile2_org = imgtile2.copy()

    imgtile2 = np.expand_dims(imgtile2, axis=2)
     # shape (1, x_pixels, y_pixels, n_bands)
    imageseg2['{}-{}-{}'.format(i, row_off2,col_off2)] = imgtile2

    if (cv2.countNonZero(imageseg2['{}-{}-{}'.format(i, row_off2,col_off2)])) > min_feature_cnt:

      trainx_list.append(imageseg['{}-{}-{}'.format(i, row_off2,col_off2)])

      trainy_list.append(imgtile2)

  cnt2 = cnt2 + 1

  i=i+1

  if i>2:
    break
    #print(imgtile.shape)

trainx = np.asarray(trainx_list)
trainy = np.asarray(trainy_list)
print(trainx.shape)
print(trainy.shape)

min_samples = min(trainx.shape[0],trainy.shape[0])

print(min_samples)

x_train, x_test, y_train, y_test = train_test_split(trainx[:min_samples], trainy[:min_samples], test_size=0.10, random_state=4)

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)
print(x_train.shape[1:])

import torch
import albumentations as A
from albumentations.pytorch import ToTensorV2

# some parameters to help in training
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

BATCH_SIZE = 1
LEARNING_RATE = 5e-5
NUM_WORKERS = 0
NUM_EPOCHS = 10
LOAD_MODEL = False
SAVE_MODEL = False
CHECKPOINT_GEN_H = "genh.pth.tar"
CHECKPOINT_GEN_Z = "genz.pth.tar"
CHECKPOINT_CRITIC_H = "critich.pth.tar"
CHECKPOINT_CRITIC_Z = "criticz.pth.tar"

import random, torch, os, numpy as np
import torch.nn as nn
import copy



# saving model weights during training.

def save_checkpoint(model, optimizer, filename="my_checkpoint.pth.tar"):
    print("=> Saving checkpoint")
    checkpoint = {
        "state_dict": model.state_dict(),
        "optimizer": optimizer.state_dict(),
    }
    torch.save(checkpoint, filename)

# loading model weights to resume training

def load_checkpoint(checkpoint_file, model, optimizer, lr):
    print("=> Loading checkpoint")
    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)
    model.load_state_dict(checkpoint["state_dict"])
    optimizer.load_state_dict(checkpoint["optimizer"])

    # If we don't do this then it will just have learning rate of old checkpoint
    # and it will lead to many hours of debugging \:
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr

# just seeds for making results reproducible.

def seed_everything(seed=42):
    os.environ["PYTHONHASHSEED"] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

seed_everything(123)

class Block(nn.Module):
    def __init__(self, in_channels, out_channels, stride):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 4, stride, 1, bias=True, padding_mode="reflect"),
            nn.InstanceNorm2d(out_channels),
            nn.LeakyReLU(0.2, inplace=True),
        )

    def forward(self, x):
        return self.conv(x)


class Discriminator(nn.Module):
    def __init__(self, in_channels=1, features=[64, 128, 256, 512]):
        super().__init__()
        self.initial = nn.Sequential(
            nn.Conv2d(
                in_channels,
                features[0],
                kernel_size=4,
                stride=2,
                padding=1,
                padding_mode="reflect",
            ),
            nn.LeakyReLU(0.2, inplace=True),
        )

        layers = []
        in_channels = features[0]
        for feature in features[1:]:
            layers.append(Block(in_channels, feature, stride=1 if feature==features[-1] else 2))
            in_channels = feature
        layers.append(nn.Conv2d(in_channels, 1, kernel_size=4, stride=1, padding=1, padding_mode="reflect"))
        self.model = nn.Sequential(*layers)

    def forward(self, x):
        x = self.initial(x)
        return self.model(x)



# this is just a test function to see if the output shape is correct for the model.

def test():
    x = torch.randn((5, 1, 256, 256))
    model = Discriminator()
    preds = model(x)
    print(preds.shape)
    print(model)

test()

class ResidualConv(nn.Module):
    def __init__(self, input_dim, output_dim, stride, padding):
        super(ResidualConv, self).__init__()

        self.conv_block = nn.Sequential(
            nn.BatchNorm2d(input_dim),
            nn.ReLU(),
            nn.Conv2d(
                input_dim, output_dim, kernel_size=3, stride=stride, padding=padding
            ),
            nn.BatchNorm2d(output_dim),
            nn.ReLU(),
            nn.Conv2d(output_dim, output_dim, kernel_size=3, padding=1),
        )
        self.conv_skip = nn.Sequential(
            nn.Conv2d(input_dim, output_dim, kernel_size=3, stride=stride, padding=1),
            nn.BatchNorm2d(output_dim),
        )

    def forward(self, x):

        return self.conv_block(x) + self.conv_skip(x)


class Upsample(nn.Module):
    def __init__(self, input_dim, output_dim, kernel, stride):
        super(Upsample, self).__init__()

        self.upsample = nn.ConvTranspose2d(
            input_dim, output_dim, kernel_size=kernel, stride=stride
        )

    def forward(self, x):
        return self.upsample(x)


class ResUnet(nn.Module):
    def __init__(self, channel, filters=[64, 128, 256, 512]):
        super(ResUnet, self).__init__()

        self.input_layer = nn.Sequential(
            nn.Conv2d(channel, filters[0], kernel_size=3, padding=1),
            nn.BatchNorm2d(filters[0]),
            nn.ReLU(),
            nn.Conv2d(filters[0], filters[0], kernel_size=3, padding=1),
        )
        self.input_skip = nn.Sequential(
            nn.Conv2d(channel, filters[0], kernel_size=3, padding=1)
        )

        self.residual_conv_1 = ResidualConv(filters[0], filters[1], 2, 1)
        self.residual_conv_2 = ResidualConv(filters[1], filters[2], 2, 1)

        self.bridge = ResidualConv(filters[2], filters[3], 2, 1)

        self.upsample_1 = Upsample(filters[3], filters[3], 2, 2)
        self.up_residual_conv1 = ResidualConv(filters[3] + filters[2], filters[2], 1, 1)

        self.upsample_2 = Upsample(filters[2], filters[2], 2, 2)
        self.up_residual_conv2 = ResidualConv(filters[2] + filters[1], filters[1], 1, 1)

        self.upsample_3 = Upsample(filters[1], filters[1], 2, 2)
        self.up_residual_conv3 = ResidualConv(filters[1] + filters[0], filters[0], 1, 1)

        self.output_layer = nn.Sequential(
            nn.Conv2d(filters[0], 1, 1, 1),
            nn.Sigmoid(),
        )

    def forward(self, x):
        # Encode
        x1 = self.input_layer(x) + self.input_skip(x)
        x2 = self.residual_conv_1(x1)
        x3 = self.residual_conv_2(x2)
        # Bridge
        x4 = self.bridge(x3)
        # Decode
        x4 = self.upsample_1(x4)
        x5 = torch.cat([x4, x3], dim=1)

        x6 = self.up_residual_conv1(x5)

        x6 = self.upsample_2(x6)
        x7 = torch.cat([x6, x2], dim=1)

        x8 = self.up_residual_conv2(x7)

        x8 = self.upsample_3(x8)
        x9 = torch.cat([x8, x1], dim=1)

        x10 = self.up_residual_conv3(x9)

        output = self.output_layer(x10)

        return output

# test function for unet.
def testu():
    img_channels = 3
    img_size = 256
    x = torch.randn((1, 3, img_size, img_size))
    #gen = Generator(img_channels, 9)
    model = ResUnet(3)
    print(model(x).shape)
    #print(gen(x))

testu()

from torch.utils.data import TensorDataset, DataLoader

x_train_tensor = torch.Tensor(x_train).permute(0, 3, 1, 2)
x_test_tensor = torch.Tensor(x_test).permute(0, 3, 1, 2)
y_train_tensor = torch.Tensor(y_train).permute(0, 3, 1, 2)
y_test_tensor = torch.Tensor(y_test).permute(0, 3, 1, 2)

print(x_train_tensor.size())
print(x_test_tensor.shape)
print(y_train_tensor.shape)
print(y_test_tensor.shape)

train_data = TensorDataset(x_train_tensor, y_train_tensor)
val_data = TensorDataset(x_test_tensor, y_test_tensor)

import torch.optim as optim
from tqdm.notebook import tqdm
from torchvision.utils import save_image

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/cyclegan

g_loss= []

criterion = nn.BCELoss()

lossmse = nn.MSELoss()

def train_fn(disc_Z, gen_Z, loader, opt_disc, opt_gen):
    H_reals = 0
    H_fakes = 0
    sum_g_loss = 0
    # tqdm loader bar
    loop = tqdm(loader, leave=True)
    i=1
    for idx, (orig_image, mask_image) in enumerate(loop):
        mask_image = mask_image.to(DEVICE)
        orig_image = orig_image.to(DEVICE)


        fake_mask_image = gen_Z(orig_image)
        D_Z_real = disc_Z(mask_image).reshape(-1)
        D_Z_fake = disc_Z(fake_mask_image).reshape(-1)

        loss_critic = -(torch.mean(D_Z_real)-torch.mean(D_Z_fake))


        disc_Z.zero_grad()
        loss_critic.backward(retain_graph = True)
        opt_disc.step()

        for p in disc_Z.parameters():
          p.data.clamp(-0.01, 0.01)


        output = disc_Z(fake_mask_image).reshape(-1)
        #loss_G_H = mse(D_H_fake, torch.ones_like(D_H_fake))
        loss_G_Z = -torch.mean(output)


        gen_Z.zero_grad()
        loss_G_Z.backward()
        opt_gen.step()


        if idx % 50 == 0:
            #fake_mask_image = torch.where(fake_mask_image<0.5, 0, 1)
            print(fake_mask_image.shape)
            fake_mask_image = torch.where(fake_mask_image<0.5, 0.0, 1.0)
            #save_image(fake_orig_image, f"saved_images/orig_image_{idx}.png")
            save_image(fake_mask_image, f"saved_images/mask_image_{idx}.png")

        loop.set_postfix(H_real=H_reals/(idx+1), H_fake=H_fakes/(idx+1))
        #sum_g_loss = sum_g_loss + G_loss
        g_loss.append(loss_G_Z)




def main():

    #define models

    #disc_H = newDiscriminator(nc=3).to(DEVICE)
    disc_Z = Discriminator(in_channels=1).to(DEVICE)
    gen_Z = ResUnet(3).to(DEVICE)

    opt_disc = optim.RMSprop(
        disc_Z.parameters(),
        lr=LEARNING_RATE,
    )

    #for discriminator
    opt_gen = optim.RMSprop(
        gen_Z.parameters(),
        lr=LEARNING_RATE,
    )



    # to resume training, make LOAD_Model in config.py to True
    if LOAD_MODEL:
        load_checkpoint(
            CHECKPOINT_GEN_Z, gen_Z, opt_gen, LEARNING_RATE,
        )
        load_checkpoint(
            CHECKPOINT_CRITIC_Z, disc_Z, opt_disc, LEARNING_RATE,
        )




    # train data loader
    loader = DataLoader(
        train_data,
        batch_size=BATCH_SIZE,
        num_workers=NUM_WORKERS,
        pin_memory=True
    )


 # train function

    for epoch in range(10):
        train_fn(disc_Z, gen_Z, loader, opt_disc, opt_gen)
        # to save the model weights
        if SAVE_MODEL:
            save_checkpoint(gen_Z, opt_gen, filename=CHECKPOINT_GEN_Z)
            save_checkpoint(disc_Z, opt_disc, filename=CHECKPOINT_CRITIC_Z)

main()

new_g = []
for x in g_loss:
  x = x.detach().cpu().numpy()
  new_g.append(x)

from matplotlib import pyplot
pyplot.plot(new_g)
